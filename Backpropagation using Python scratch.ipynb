{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0829d007",
   "metadata": {},
   "source": [
    "## Backpropagation in Multilayer perceptron using Python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "646b42d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adee735d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the dataset here\n",
    "X = np.array(([1, 9], [2, 7], [3, 6],[4, 56]), dtype=float)\n",
    "y = np.array(([96], [89],[75],[66]), dtype=float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51871fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= X/np.amax(X,axis=0)\n",
    "y=y/100 #max test score being 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4622b4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.25       0.16071429]\n",
      " [0.5        0.125     ]\n",
      " [0.75       0.10714286]\n",
      " [1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65f6f4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.96]\n",
      " [0.89]\n",
      " [0.75]\n",
      " [0.66]]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70bfd3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2950715126059021\n",
      "Loss: 0.028724680525595347\n",
      "Loss: 0.02299903205144041\n",
      "Loss: 0.02106036964232231\n",
      "Loss: 0.019504042739172518\n",
      "Loss: 0.01804765167146793\n",
      "Loss: 0.01667266811702363\n",
      "Loss: 0.015392503825113962\n",
      "Loss: 0.014218928833495511\n",
      "Loss: 0.01315709081249821\n",
      "Loss: 0.012206014669345006\n",
      "Loss: 0.011360244728712509\n",
      "Loss: 0.01061154275833293\n",
      "Loss: 0.009950294234702955\n",
      "Loss: 0.009366532842402714\n",
      "Loss: 0.008850610156273551\n",
      "Loss: 0.008393583892535169\n",
      "Loss: 0.007987405123796615\n",
      "Loss: 0.007624973488752107\n",
      "Loss: 0.007300112593121696\n",
      "Loss: 0.007007501892095855\n",
      "Loss: 0.0067425886881082985\n",
      "Loss: 0.0065014947295851866\n",
      "Loss: 0.006280925697034971\n",
      "Loss: 0.006078087867376351\n",
      "Loss: 0.005890613787125364\n",
      "Loss: 0.005716497343451608\n",
      "Loss: 0.005554037831036223\n",
      "Loss: 0.005401792222829879\n",
      "Loss: 0.005258534701450835\n",
      "Loss: 0.005123222491875975\n",
      "Loss: 0.004994967091552231\n",
      "Loss: 0.004873010082863017\n",
      "Loss: 0.004756702812866396\n",
      "Loss: 0.004645489324152812\n",
      "Loss: 0.004538892012322783\n",
      "Loss: 0.004436499567264888\n",
      "Loss: 0.004337956826430699\n",
      "Loss: 0.004242956229043061\n",
      "Loss: 0.004151230611546569\n",
      "Loss: 0.004062547127729243\n",
      "Loss: 0.003976702112951624\n",
      "Loss: 0.003893516741886953\n",
      "Loss: 0.0038128333540674315\n",
      "Loss: 0.003734512342187213\n",
      "Loss: 0.003658429515249945\n",
      "Loss: 0.0035844738628744485\n",
      "Loss: 0.0035125456588919364\n",
      "Loss: 0.0034425548522021253\n",
      "Loss: 0.003374419701051598\n",
      "Loss: 0.0033080656137406635\n",
      "Loss: 0.0032434241644901684\n",
      "Loss: 0.0031804322580000666\n",
      "Loss: 0.0031190314202648763\n",
      "Loss: 0.0030591671966077464\n",
      "Loss: 0.0030007886407606727\n",
      "Loss: 0.0029438478812419916\n",
      "Loss: 0.002888299753335086\n",
      "Loss: 0.0028341014867142504\n",
      "Loss: 0.0027812124402443366\n",
      "Loss: 0.0027295938767407176\n",
      "Loss: 0.002679208771549644\n",
      "Loss: 0.0026300216497245274\n",
      "Loss: 0.0025819984473547442\n",
      "Loss: 0.0025351063932705737\n",
      "Loss: 0.002489313907917345\n",
      "Loss: 0.0024445905166781312\n",
      "Loss: 0.0024009067753395833\n",
      "Loss: 0.002358234205749935\n",
      "Loss: 0.002316545240020303\n",
      "Loss: 0.0022758131718783646\n",
      "Loss: 0.0022360121140027034\n",
      "Loss: 0.00219711696035279\n",
      "Loss: 0.0021591033526683124\n",
      "Loss: 0.002121947650446008\n",
      "Loss: 0.0020856269038162173\n",
      "Loss: 0.0020501188288378183\n",
      "Loss: 0.0020154017848115218\n",
      "Loss: 0.001981454753280275\n",
      "Loss: 0.0019482573184429288\n",
      "Loss: 0.0019157896487559754\n",
      "Loss: 0.0018840324795384906\n",
      "Loss: 0.0018529670964290413\n",
      "Loss: 0.0018225753195718803\n",
      "Loss: 0.001792839488432437\n",
      "Loss: 0.0017637424471618698\n",
      "Loss: 0.0017352675304461004\n",
      "Loss: 0.0017073985497876344\n",
      "Loss: 0.0016801197801793227\n",
      "Loss: 0.0016534159471376152\n",
      "Loss: 0.0016272722140698145\n",
      "Loss: 0.0016016741699555867\n",
      "Loss: 0.0015766078173270151\n",
      "Loss: 0.0015520595605354336\n",
      "Loss: 0.0015280161942954873\n",
      "Loss: 0.0015044648924993653\n",
      "Loss: 0.0014813931972953414\n",
      "Loss: 0.0014587890084262424\n",
      "Loss: 0.0014366405728239464\n",
      "Loss: 0.0014149364744568895\n",
      "Input: [[0.25       0.16071429]\n",
      " [0.5        0.125     ]\n",
      " [0.75       0.10714286]\n",
      " [1.         1.        ]]\n",
      "Actual Output: [[0.96]\n",
      " [0.89]\n",
      " [0.75]\n",
      " [0.66]]\n",
      "Loss: 0.0013936656244276165\n",
      "\n",
      "\n",
      "Predicted Output: [[0.91257956]\n",
      " [0.86071471]\n",
      " [0.79963315]\n",
      " [0.65778932]]\n"
     ]
    }
   ],
   "source": [
    "#Creating the neural network\n",
    "class Neural_Network(object):\n",
    "    def __init__(self):\n",
    "        #Parameters initialisation\n",
    "        self.inputsize = 2\n",
    "        self.outputsize = 1\n",
    "        self.hiddensize = 4\n",
    "        \n",
    "        #weights\n",
    "        self.w1 = np.random.randn(self.inputsize, self.hiddensize) # (3x2) weight matrix from input to hidden layer\n",
    "        self.w2 = np.random.randn(self.hiddensize, self.outputsize) # (3x1) weight matrix from hidden to output layer\n",
    "        \n",
    "    def Forward(self, X):\n",
    "        #forward propogation through the network\n",
    "        self.z = np.dot(X, self.w1) #dot product of X (input) and first set of weights (3x2)\n",
    "        self.z2 = self.Sigmoid(self.z) #activation function\n",
    "        self.z3 = np.dot(self.z2, self.w2) #dot product of hidden layer (z2) and second set of weights (3x1)\n",
    "        output = self.Sigmoid(self.z3)\n",
    "        return output\n",
    "        \n",
    "    def Sigmoid(self, s, deriv=False):\n",
    "        if (deriv == True):\n",
    "            return s * (1 - s)\n",
    "        return 1/(1 + np.exp(-s))\n",
    "    \n",
    "    def Backward(self, X, y, output):\n",
    "        #backward propogate through the network\n",
    "        self.output_error = y - output # error in output\n",
    "        self.output_delta = self.output_error * self.Sigmoid(output, deriv=True)\n",
    "        \n",
    "        self.z2_error = self.output_delta.dot(self.w2.T) #z2 error: how much our hidden layer weights contribute to output error\n",
    "        self.z2_delta = self.z2_error * self.Sigmoid(self.z2, deriv=True) #applying derivative of sigmoid to z2 error\n",
    "        \n",
    "        self.w1 += X.T.dot(self.z2_delta) # adjusting first set (input -> hidden) weights\n",
    "        self.w2 += self.z2.T.dot(self.output_delta) # adjusting second set (hidden -> output) weights\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        output = self.Forward(X)\n",
    "        self.Backward(X, y, output)\n",
    "        \n",
    "objNeural = Neural_Network()\n",
    "\n",
    "for i in range(1000): #Number of eoochs= 1000\n",
    "    if (i % 10 == 0):\n",
    "        print(\"Loss: \" + str(np.mean(np.square(y - objNeural.Forward(X)))))\n",
    "    objNeural.train(X, y)\n",
    "        \n",
    "print(\"Input: \" + str(X))\n",
    "print(\"Actual Output: \" + str(y))\n",
    "print(\"Loss: \" + str(np.mean(np.square(y - objNeural.Forward(X)))))\n",
    "print(\"\\n\")\n",
    "print(\"Predicted Output: \" + str(objNeural.Forward(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaded874",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
