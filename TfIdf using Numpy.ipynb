{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61a5ed01",
   "metadata": {},
   "source": [
    "### TF= Term Frequency\n",
    "### IDF- Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81bcc2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The filename, directory name, or volume label syntax is incorrect.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pip install heapq_max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d539059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb27db8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "document= urllib.request.urlopen('https://en.wikipedia.org/wiki/2022_Commonwealth_Games')\n",
    "document= document.read()\n",
    "document_html= bs.BeautifulSoup(document,'lxml')\n",
    "document_paragraphs= document_html.find_all('p')\n",
    "document_text=''\n",
    "for para in document_paragraphs:\n",
    "    document_text+= para.text\n",
    "corpus= nltk.sent_tokenize(document_text)\n",
    "for i in range(len(corpus)):\n",
    "    corpus[i]= corpus[i].lower()\n",
    "    corpus[i]= re.sub(r'\\W',' ',corpus[i])\n",
    "    corpus[i]= re.sub(r'\\s+',' ',corpus[i])\n",
    "\n",
    "\n",
    "wordfreq={}\n",
    "for sentence in corpus:\n",
    "    tokens= nltk.word_tokenize(sentence)\n",
    "    for token in tokens:\n",
    "        if token not in wordfreq.keys():\n",
    "            wordfreq[token]=1\n",
    "        else:\n",
    "            wordfreq[token]+=1\n",
    "\n",
    "import heapq\n",
    "most_freq= heapq.nlargest(200, wordfreq, key=wordfreq.get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e76ee246",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_idf_values={}\n",
    "for token in most_freq:\n",
    "    doc_containing_word=0\n",
    "    for doc in corpus:\n",
    "        if token in nltk.word_tokenize(doc):\n",
    "                doc_containing_word+=1\n",
    "    word_idf_values[token]= np.log(len(corpus)/1+ doc_containing_word)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2ce09dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tf_values={}\n",
    "for token in most_freq:\n",
    "    sent_tf_vector=[]\n",
    "    for doc in corpus:\n",
    "        doc_freq=0\n",
    "        for word in nltk.word_tokenize(doc):\n",
    "            if token==word:\n",
    "                doc_freq+=1\n",
    "        word_tf= doc_freq/len(nltk.word_tokenize(doc))\n",
    "        sent_tf_vector.append(word_tf)\n",
    "    word_tf_values[token]= sent_tf_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cc9b285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.047619047619047616, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07142857142857142, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05555555555555555, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(word_tf_values[token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ffa23a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=[]\n",
    "for token in word_tf_values.keys():\n",
    "    tfidf_sentences=[]\n",
    "    for tf_sentence in word_tf_values[token]:\n",
    "        tfidf_score= tf_sentence* word_idf_values[token]\n",
    "        tfidf_sentences.append(tfidf)\n",
    "    tfidf.append(tfidf_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bec6ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_model= np.array(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cb09ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf_idf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e34a37e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
